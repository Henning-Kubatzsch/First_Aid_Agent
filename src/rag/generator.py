# Wrap llama_cpp.Llama; builds prompt + generates test (temperature, max_tokens)
